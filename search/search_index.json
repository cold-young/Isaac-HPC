{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#isaac-hpc","title":"Isaac-HPC","text":"<p>This repository contains tutorial documentation on how to use NVIDIA Isaac Sim on HPC systems, particularly KISTI's Neuron system.  To this end, I have created some Python scripts to run Isaac Sim/Lab on HPC systems. + Singularity</p> <p></p> <p>Contributor: Chanyoung Ahn </p>"},{"location":"#todo","title":"TODO","text":"<ul> <li>[x] Documentation for using HPC Nurion (CPU).</li> <li>[x] Documentation for using HPC Neuron (GPU).</li> <li>[x] Tutorial for using AI model with multiple GPU w/ Neuron</li> <li>[ ] Tutorial for using Isaac Sim on HPC Neuron.</li> <li>[ ] Tutorial for using Isaac Lab with Singularity for HPC learning.</li> </ul>"},{"location":"#sturucture","title":"Sturucture","text":""},{"location":"#docsnurion_kisti","title":"docs/Nurion_KISTI","text":"<ul> <li>Documents for using HPC Nurion of KISTI.</li> <li>reference: https://github.com/cold-young?tab=repositories</li> </ul>"},{"location":"Neuron_KISTI/","title":"Neuron - KISTI Documentation","text":"<ul> <li>contribution: Chanyoung Ahn</li> </ul> <p>This document outlines how to use NVIDIA Isaac Sim on HPC systems, particularly KISTI's Neuron system.</p> <ul> <li>Overall Guideline: Link</li> <li>Neuron system main page: Link</li> </ul>"},{"location":"Neuron_KISTI/#login","title":"Login","text":"<ul> <li>Use myksc </li> <li>Or ssh <pre><code> ssh -l &lt;user ID&gt; neuron01.ksc.re.kr -p 22\n</code></pre></li> </ul>"},{"location":"Neuron_KISTI/#general-policy","title":"General Policy","text":"<ul> <li>Home directory <code>/home01</code>: 64GB, # of files &lt; 100K </li> <li> <p>Scratch directory <code>/scratch</code>: 100TB, # of files &lt; 4M. Files not accessed in 15 days will be automatically deleted! </p> </li> <li> <p>Check your directory    <pre><code>neuroninfo\n</code></pre></p> </li> <li> <p>Short command for scratch directory   <pre><code>  cds # scratch/$USER_ID\n</code></pre></p> </li> </ul>"},{"location":"Neuron_KISTI/#just-test-in-debug-node","title":"Just Test in Debug Node!","text":"<pre><code>ssh glogin01\nssh gdebug01\n\nnvidia-smi\napptainer exec --nv myenv.sif python -c \"import torch; print(torch.cuda.is_available())\"\n\n./(\uc2e4\ud589\ud30c\uc77c) (\uc635\uc158)\n</code></pre>"},{"location":"Neuron_KISTI/#interactive-job","title":"Interactive job","text":"<ul> <li>Cannot use in debug node <pre><code>salloc --partition=cas_v100_4 --nodes=2 --ntasks-per-node=2 --gres=gpu:2 --comment python\nsalloc -p amd_a100nv_8 -N 1 --gres=gpu:1 -c 8 --mem=32G -t 02:00:00 --comment pytorch # for learning isaacsim (slow version)\nsalloc -p eme_h200nv_8 -N 1 --gres=gpu:2 -c 16 --mem=32G -t 00:30:00 --comment pytorch\n# Refer to the Table of SBATCH option name per application \n\nsrun ./(\uc2e4\ud589\ud30c\uc77c) (\uc2e4\ud589\uc635\uc158)\n\nexit \n\n# Delete the job\nscancel [JOBID]\n\n# Job monitoring\nsinfo\nsqueue -u &lt;YOUR ID&gt;\n</code></pre></li> </ul>"},{"location":"Neuron_KISTI/#see-other-tutorial-documents","title":"See other tutorial documents!","text":"<ul> <li>build_your_env</li> <li>Run model with multiple GPU</li> </ul>"},{"location":"Neuron_KISTI/Exmaple_schedule/","title":"Job Examples","text":""},{"location":"Neuron_KISTI/Exmaple_schedule/#job-submission-example","title":"Job submission Example","text":""},{"location":"Neuron_KISTI/Exmaple_schedule/#sbatch-exmaple-with-ddp-torchrun-experimental","title":"Sbatch Exmaple with DDP (Torchrun) - Experimental","text":"<pre><code>#!/bin/bash\n#SBATCH -J JOB_NAME\n#SBATCH -p amd_h200nv_8 \n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1         \n#SBATCH --cpus-per-task=16\n#SBATCH --gres=gpu:2\n#SBATCH --time=00:40:00\n#SBATCH --comment=pytorch\n#SBATCH -o /scratch/YOUR_ID/logs/%x_%j.out\n#SBATCH -e /scratch/YOUR_ID/logs/%x_%j.err\n#SBATCH --mail-type=END\n#SBATCH --mail-user=YOUR_EMAIL\n\nset -euo pipefail\nulimit -n 65535 || true\n\nexport OMP_NUM_THREADS=1\nexport MKL_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=1\nexport NUMEXPR_NUM_THREADS=1\n\nPY=/home01/[$USER]/.conda/envs/reg/bin/python\nWORKDIR=/scratch/[$USER]/[$YOUR_PROJECT]\nTRAIN_SCRIPT=[$YOUR_TRAIN_FILE].py\n\necho \"[INFO] Host: $(hostname)\"\necho \"[INFO] Job : ${SLURM_JOB_ID:-NA}\"\necho \"[INFO] Using PY: $PY\"\n$PY -V\n\nnvidia-smi\necho \"[INFO] CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-&lt;unset&gt;}\"\n\n# ---- decide nproc_per_node robustly ----\nif [[ -n \"${CUDA_VISIBLE_DEVICES:-}\" ]]; then\n  NPROC_PER_NODE=$(echo \"$CUDA_VISIBLE_DEVICES\" | awk -F',' '{print NF}')\nelse\n  # 2) Slurm GPU \n  NPROC_PER_NODE=\"${SLURM_GPUS_ON_NODE:-${SLURM_GPUS_PER_NODE:-2}}\"\nfi\nexport NPROC_PER_NODE\necho \"[INFO] NPROC_PER_NODE=$NPROC_PER_NODE\"\n\n# ---- quick torch sanity ----\n$PY - &lt;&lt;'PY'\nimport os, torch\nprint(\"[INFO] torch\", torch.__version__)\nprint(\"[INFO] cuda available:\", torch.cuda.is_available())\nprint(\"[INFO] device_count:\", torch.cuda.device_count())\nprint(\"[INFO] CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\nif torch.cuda.is_available() and torch.cuda.device_count() &gt; 0:\n    for i in range(torch.cuda.device_count()):\n        print(f\"[INFO] cuda:{i} -&gt;\", torch.cuda.get_device_name(i))\nPY\necho \"[INFO] which python: $(which python || true)\"\necho \"[INFO] which torchrun: $(which torchrun || true)\"\n\"$PY\" -c \"import torch, sys; print('torch', torch.__version__); print('python', sys.executable)\"\n\"$PY\" -c \"import torch.distributed.run as r; print('torch.distributed.run OK')\"\n\n# ---- stage DATA into node-local ----\nBASE_TMP=\"${SLURM_TMPDIR:-/tmp}\"\nDATA_SRC=/scratch/[$USER]/[$YOUR_DATASET]\nDATA_DST=\"$BASE_TMP/[$YOUR_TMP_DATASET]\"\n\necho \"[INFO] ---- STAGE DATA START ----\"\ndate\n[[ -d \"$DATA_SRC\" ]] || { echo \"[ERR] DATA_SRC missing: $DATA_SRC\"; exit 1; }\n[[ -n \"$DATA_DST\" ]] || { echo \"[ERR] DATA_DST empty\"; exit 1; }\n[[ \"$DATA_DST\" == *\"/YOUR_DATASET\" ]] || { echo \"[ERR] DATA_DST looks unsafe: $DATA_DST\"; exit 1; }\n\nmkdir -p \"$DATA_DST\"\ntime rsync -a --delete \"$DATA_SRC\"/ \"$DATA_DST\"/\ndate\necho \"[INFO] ---- STAGE DATA END ----\"\n\nexport DATA_DIR=\"$DATA_DST\"\necho \"[INFO] DATA_DIR=$DATA_DIR\"\nls -l \"$DATA_DST\" | head -n 5 || true\n\ncd \"$WORKDIR\"\necho \"[INFO] PWD=$(pwd)\"\necho \"[INFO] ---- TRAIN START ----\"\ndate\nexport PYTHONUNBUFFERED=1\n\ntime $PY -m torch.distributed.run \\\n  --standalone --nnodes=1 --nproc_per_node=\"$NPROC_PER_NODE\" [$YOUR_FILE].py\n\ndate\necho \"[INFO] ---- TRAIN END ----\"\n</code></pre>"},{"location":"Neuron_KISTI/Exmaple_schedule/#pytorchddp-official-guide","title":"PytorchDDP (Official Guide)","text":"<ol> <li> <p>Single Node with 2GPU <pre><code>#!/bin/bash -l\n#SBATCH -J PytorchDDP\n#SBATCH -p cas_v100_4\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:2\n#SBATCH --comment pytorch\n#SBATCH --time 1:00:00\n#SBATCH -o %x.o%j\n#SBATCH -e %x.e%j\n\n# Configuration\ntraindata='{path}'\nmaster_port=\"$((RANDOM%55535+10000))\"\n\n# Load software\nconda activate pytorchDDP\n\n# Launch one SLURM task, and use torch distributed launch utility\n# to spawn training worker processes; one per GPU\nsrun -N 1 -n 1 python main.py -a config \\\n                                --dist-url \"tcp://127.0.0.1:${master_port}\" \\\n                                --dist-backend 'nccl' \\\n                                --multiprocessing-distributed \\\n                                --world-size $SLURM_TASKS_PER_NODE \\\n                                --rank 0 \\\n                                $traindata\n</code></pre></p> </li> <li> <p>Multi Node with 2GPU <pre><code>#!/bin/bash -l\n#SBATCH -J PytorchDDP\n#SBATCH -p cas_v100_4\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --comment pytorch\n#SBATCH --time 10:00:0\n#SBATCH -o %x.o%j\n#SBATCH -e %x.e%j\n\n# Load software list\nmodule load {module name}\nconda activate {conda name}\n\n# Setup node list\nnodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names\nnodes_array=( $nodes )\nmaster_node=${nodes_array[0]}\nmaster_addr=$(srun --nodes=1 --ntasks=1 -w $master_node hostname --ip-address)\nmaster_port=$((RANDOM%55535+10000))\nworker_num=$(($SLURM_JOB_NUM_NODES))\n\n# Loop over nodes and submit training tasks\nfor ((  node_rank=0; node_rank&lt;$worker_num; node_rank++ )); do\n          node=${nodes_array[$node_rank]}\n          echo \"Submitting node # $node_rank, $node\"\n          # Launch one SLURM task per node, and use torch distributed launch utility\n          # to spawn training worker processes; one per GPU\n          srun -N 1 -n 1 -w $node python main.py -a $config \\\n                                                             --dist-url tcp://$master_addr:$master_port \\\n                                                             --dist-backend 'nccl' \\\n                                                            --multiprocessing-distributed \\\n                                                            --world-size $SLURM_JOB_NUM_NODES \\\n                                                            --rank $node_rank &amp;\n\n          pids[${node_rank}]=$!\ndone\n\n# Wait for completion\nfor pid in ${pids[*]}; do\n          wait $pid\ndone\n</code></pre></p> </li> </ol>"},{"location":"Neuron_KISTI/build_your_env/","title":"Build own's Environment (IsaacSim / IsaacLab) in Neuron System","text":"<ul> <li>Caution: Isaacsim only is supported by RTX </li> </ul>"},{"location":"Neuron_KISTI/build_your_env/#conda","title":"Conda","text":"<pre><code>$ module load gcc/10.2.0 cuda/11.4 cudampi/openmpi-4.1.1 python/3.7.1 cmake/3.16.9\n$ conda create -n my_pytorch  \n$ source activate my_pytorch\n(my_pytorch) $  \n\n\n# If you want to install torch / \n# for Python3.11\npip install torch==2.7.0 torchvision==0.22.0 --index-url https://download.pytorch.org/whl/cu128\n\n# for Python3.10\npip install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu118\n\n# IsaacSim # Recommmand: Just use image!\npip install \"isaacsim[all,extscache]==5.1.0\" --extra-index-url https://pypi.nvidia.com\n\n## Install check\npython - &lt;&lt;'PY'\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name(0))\nPY\n</code></pre>"},{"location":"Neuron_KISTI/build_your_env/#docker-apptainer","title":"Docker / Apptainer","text":""},{"location":"Neuron_KISTI/build_your_env/#build-your-isaacsim-environment-with-apptainer","title":"Build Your IsaacSim Environment with Apptainer","text":"<p>Load singularity module in Neuron system <pre><code>$ module load singularity/3.11.0\n# or\n$ $HOME/.bash_profile\nexport PATH=$PATH:/apps/applications/singularity/3.11.0/bin/\n# check\nsingluarity --version\n</code></pre></p>"},{"location":"Neuron_KISTI/build_your_env/#local-install-isaac-sim-container","title":"(Local) Install Isaac Sim container","text":"<ul> <li>Caution: Need NVIDIA Docker in your local machine!  -- If you use docker in KIST, please use Docker Pro for commercial license.  <pre><code>docker pull nvcr.io/nvidia/isaac-sim:5.1.0\n</code></pre></li> </ul>"},{"location":"Neuron_KISTI/build_your_env/#change-docker-image-in-isaac-lab-or-dex_soldering-internal-repo-for-kist","title":"Change Docker image in Isaac Lab or dex_soldering (internal repo for KIST)","text":"<ol> <li> <p>Install <code>IsaacLab</code> or <code>dex_soldering</code> repo.     <pre><code>cd $YOUR_PROJECT_DIR\n</code></pre></p> </li> <li> <p><code>/docker/cluster/.env.cluster</code></p> </li> <li> <p>Change [$YOUR_PROJECT] to <code>isaaclab</code> or <code>dex_soldering</code> <pre><code># Job scheduler used by cluster.\n# Currently supports PBS and SLURM\nCLUSTER_JOB_SCHEDULER=SLURM\n\n# e.g. /cluster/scratch/$USER/docker-isaac-sim\nCLUSTER_ISAAC_SIM_CACHE_DIR=/scratch/$USER/isaac_cache/isaacsim-5.1.0/docker-isaac-sim\n\n# Isaac Lab directory on the cluster (has to end on isaaclab)\nCLUSTER_ISAACLAB_DIR=/scratch/$USER/[$YOUR_PROJECT]/isaaclab\n\n# Cluster login\nCLUSTER_LOGIN=[$YOUR_KISTI_ID]@neuron.ksc.re.kr\n\n# Cluster scratch directory to store the SIF file\n# CLUSTER_SIF_PATH=/some/path/on/cluster/\nCLUSTER_SIF_PATH=/scratch/$USER/dex_soldering/containers\n# Remove the temporary isaaclab code copy after the job is done\nREMOVE_CODE_COPY_AFTER_JOB=false\n# Python executable within Isaac Lab directory to run with the submitted job\nCLUSTER_PYTHON_EXECUTABLE=scripts/reinforcement_learning/rsl_rl/train.py\n\nTMPDIR=${SLURM_TMPDIR:-/tmp}/isaaclab_${SLURM_JOB_ID}\n</code></pre></p> </li> <li> <p><code>/docker/cluster/cluster_interface.sh</code> <pre><code>...\nsubmit_job() {\n\n    echo \"[INFO] Arguments passed to job script ${@}\"\n\n    case $CLUSTER_JOB_SCHEDULER in\n        \"SLURM\")\n            CMD='bash -l' ## Add\n            job_script_file=submit_job_slurm.sh\n            ;;\n        \"PBS\")\n            CMD=bash ## Add \n            job_script_file=submit_job_pbs.sh\n            ;;\n        *)\n            echo \"[ERROR] Unsupported job scheduler specified: '$CLUSTER_JOB_SCHEDULER'. Supported options are: ['SLURM', 'PBS']\"\n            exit 1\n            ;;\n    esac\n\n    ssh $CLUSTER_LOGIN \"cd $CLUSTER_ISAACLAB_DIR &amp;&amp; $CMD $CLUSTER_ISAACLAB_DIR/docker/cluster/$job_script_file \\\"$CLUSTER_ISAACLAB_DIR\\\" \\\"isaac-lab-$profile\\\" ${@}\"\n}\n...\n</code></pre></p> </li> </ol>"},{"location":"Neuron_KISTI/build_your_env/#build-and-push-a-docker-image","title":"Build and push a docker image","text":"<ol> <li> <p>First, build a new image</p> <pre><code>python docker/container.py start\n</code></pre> </li> <li> <p>push the created image to the server.</p> <pre><code>./docker/cluster/cluster_interface.sh push base\n</code></pre> </li> <li> <p>check your docker image (in local)    <pre><code>docker images | grep -i isaac\n</code></pre></p> </li> </ol> <p>If you want to create a custom image with a different name, replace the name <code>base</code> with other names. Also rename the files (i.e. .<code>env.base</code>) accordingly.</p>"},{"location":"Neuron_KISTI/build_your_env/#docker-image-to-local-apptainer-sif-conversion","title":"Docker image to Local Apptainer SIF conversion","text":"<pre><code>docker save isaac-lab-base:latest -o isaac-lab-base_latest.tar\nls -lh isaac-lab-base_latest.tar\n</code></pre> <pre><code># Apptainer build \nsudo apptainer build isaac-lab-base_isaacsim5.1.0.sif docker-archive://isaac-lab-base_latest.tar\n\n# Singulurity build\nsudo singularity build isaac-lab-base_isaacsim5.1.0.sif docker-archive://isaac-lab-base_latest.tar\n</code></pre>"},{"location":"Neuron_KISTI/build_your_env/#check-the-sif-file","title":"Check the SIF file","text":"<pre><code>ls -lh isaac-lab-base_isaacsim5.1.0.sif\napptainer inspect isaac-lab-base_isaacsim5.1.0.sif | head\n</code></pre>"},{"location":"Neuron_KISTI/build_your_env/#test-10-epoch-in-hpc-neuron-system-with-isaac-lab","title":"Test 10 epoch in HPC Neuron system with Isaac Lab","text":"<ul> <li>We cannot use <code>video</code> option in HPC system...... (only RT core supported -- rtx series) <pre><code>singularity exec --nv --containall \\\n  -B \"$ISAACLAB_SRC:/workspace/isaaclab:rw\" \\\n  -B \"$KIT_DATA:/isaac-sim/kit/data:rw\" \\\n  -B \"$KIT_CACHE:/isaac-sim/kit/cache:rw\" \\\n  -B \"$KIT_LOGS:/isaac-sim/kit/logs:rw\" \\\n  \"$SIF\" \\\n  bash -lc '\n    set -e\n    cd /workspace/isaaclab\n\n    touch /isaac-sim/kit/data/__rw_test__ &amp;&amp; rm -f /isaac-sim/kit/data/__rw_test__\n\n    /isaac-sim/python.sh scripts/reinforcement_learning/rsl_rl/train.py \\\n      --task Isaac-Velocity-Rough-Anymal-C-v0 \\\n      --headless \\\n      --num_envs 64 \\\n      --max_iterations 10 \\ \n  '\n</code></pre></li> </ul>"},{"location":"Nurion_KISTI/","title":"Study about HPC","text":""},{"location":"Nurion_KISTI/#hpc","title":"HPC?","text":""},{"location":"Nurion_KISTI/#kisti-nurion","title":"KISTI Nurion","text":"<ul> <li>#1_Nurion Guide</li> <li>#2_HPC_Job Tutorial</li> <li>#3_PaScaL_TDMA_env</li> <li>#4_SuperLU_dist</li> </ul>"},{"location":"Nurion_KISTI/#utils","title":"utils","text":"<ul> <li>README</li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/","title":"NURION GUIDE (KOR.)","text":"<p>Date: 2024.05.21 (Tue)  Writer: Chanyoung Ahn (cold-young)</p> <ul> <li>KOR version \uc791\uc131 \ud6c4, ENG version \uc791\uc131 \uc608\uc815.</li> <li>Reference: Nurion-guide</li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#nurion","title":"NURION","text":"<ul> <li>KISTI \uc288\ud37c\ucef4\ud4e8\ud130 5\ud638\uae30, \ub9ac\ub205\uc2a4 \uae30\ubc18 \ucd08\ubcd1\ub82c \ud074\ub7ec\uc2a4\ud130 \uc2dc\uc2a4\ud15c</li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#_1","title":"\uacc4\uc0b0\ub178\ub4dc","text":"<ol> <li> <p>KNL (manycore) node</p> <ol> <li>Processor</li> <li>Intel Xeon Phi 7250 processor / 8,305 nodes  </li> <li>Perfonrmance/CPU: 3.0464 TFLOPS </li> <li>Core/CPU: 68 </li> <li> <p>Node/CPU: 1 </p> </li> <li> <p>Main Memory </p> </li> <li>Memory/Node: 96 GB </li> </ol> </li> <li> <p>SKL (CPU-only) node</p> </li> <li>Intel Xeon Gold 6148 (Skylake) / 132 nodes</li> </ol>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#login-node","title":"Login node","text":""},{"location":"Nurion_KISTI/01_HPC_overall_guide/#ssh","title":"SSH","text":"<pre><code>ssh -l &lt;UserID&gt; nurion.ksc.re.kr\nssh -l &lt;UserID&gt; nurion.ksc.re.kr -p 22\n</code></pre>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#myksc","title":"MyKSC","text":"<ul> <li>https://my.ksc.re.kr</li> <li>Add <code>ttyd(terminal)</code> app</li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#send-receive-files","title":"Send &amp; Receive Files","text":"<ul> <li>FTP Client  <pre><code>ftp nurion-dm.ksc.re.kr\n# or\nsftp [USERID@]nurion-dm.ksc.re.kr [-p22]\n</code></pre></li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#monitor-sru-time","title":"Monitor SRU Time","text":"<pre><code>isam\n</code></pre>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#file-system-quater-policy","title":"File system &amp; quater policy","text":"<ul> <li>Provide two file systems, <code>/home01/$USER</code> and <code>/scratch/$USER</code>(<code>$ cds</code>).</li> <li>Performance of Home directory is limited. We should use scratch directory when use any computation..</li> <li><code>/home01/$USER</code>: 64GB </li> <li><code>/scratch/$USER</code>: 100TB / Delete files have not been used for 15 days</li> </ul> <pre><code>lfs quota -h /home01 # check file system disk\n</code></pre>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#how-to-set-module-compiler","title":"How to set module &amp; compiler","text":"<ol> <li>Compiler &amp; modulus Setting</li> <li><code>module avail</code>: view all module list</li> <li><code>module load [module name] [module name]</code> .. </li> <li><code>module list</code>: view install module list</li> <li> <p><code>module purge</code>: remove all used module `</p> </li> <li> <p>Serial Program Compile</p> </li> <li>Intel Compiler</li> <li><code>icc</code> or <code>icpc</code>: C / C++ Language</li> <li><code>ifort</code>: F77 / F90 Language</li> <li> <p>Compiler Option</p> <ul> <li><code>-O</code>[1|2|3]: Object optimization</li> <li><code>-ip</code>, <code>-ipo</code>: \ud504\ub85c\uc2dc\uc800 \uac04 \ucd5c\uc801\ud654</li> <li><code>-qopt_report=[0|1|2|3|4]</code>:\ubca1\ud130 \uc9c4\ub2e8 \uc815\ubcf4\uc758 \uc591\uc744 \uc870\uc808</li> <li><code>xCORE-AVX512</code>, <code>xMIC-AVX512</code>: support CPU with 512bits register (for SKL node) / support MIC with 512bits register (for KNL node)</li> <li><code>fast</code>: <code>-O3 -ipo -no-prec-div -static</code>, <code>-fp-model fast=2</code> macro</li> <li><code>-shared/-shared-inel/-i_dynamic</code>: link shared libraries</li> <li><code>-g -fp</code>: provide debugging information</li> <li><code>-qopenmp</code>: Use multi-thread code based on OpenMP</li> <li><code>-openmp_report=[0|1|2]</code>: Change OpenMP parallelization level</li> <li> <p><code>-fPIC, fpic</code>: Compile with PIC (Position Independent Code)</p> </li> <li> <p>Recommend Options</p> </li> <li>SKL: <code>-O3 -fPIC -xCORE-AVX512</code></li> <li>KNL: <code>-O3 -fPIC -xMIC-AVX512</code></li> <li>SKL &amp; KNL: <code>-O3 -fPIC -xCOMMON-AVX512</code> </li> </ul> </li> <li> <p>Parallel Program Compile</p> </li> <li>OpenMP Compiler</li> <li><code>icc</code>, <code>icpc</code>, <code>ifort</code>: C / C++ / F77/F90 <code>-qopenmp</code></li> <li><code>gcc</code>, <code>g++</code>, <code>gfortran</code>: C / C++ / F77/F90 <code>-fopenmp</code></li> </ol> <p>Example:     <pre><code>modle load craype-mic-knl intel/18.0.3\nicc -o test_omp.ext -qopenmp -O3 -fPIC -xMIC-AVX512 test_omp.c\n</code></pre></p> <ul> <li>MPI Compiler</li> </ul> <p></p> <p>Example:     <pre><code>modle load craype-mic-knl intel/18.0.3 impl/18.0.3\nmpiicc -o test_omp.ext -O3 -fPIC -xMIC-AVX512 test_omp.c\nmpirun -np 2 ./text_mpi.exe\n</code></pre></p>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#portable-batch-system-pbs-scheduler","title":"Portable Batch System (PBS) Scheduler","text":"<ul> <li>One node for one user</li> <li><code>/scratch/$USER</code> </li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#monitoring","title":"Monitoring","text":""},{"location":"Nurion_KISTI/01_HPC_overall_guide/#1-submit-batch-job","title":"1. Submit Batch Job","text":"<ul> <li>script examples <code>/apps/shell/home/job_examples</code></li> <li>PBS scheduler options</li> <li><code>PBS -V</code>: remain current environment variable</li> <li><code>PBS -N</code>: name current job</li> <li><code>PBS -q</code>: queue for job </li> </ul> <p>MPI (IntelMPI) Example (mpi.sh):     <pre><code>#!/bin/sh\n#PBS -N IntelMPI_job\n#PBS -V\n#PBS -q normal\n#PBS -A\n#PBS -l select=4:ncpus=64:mpiprocs=64\n#PBS -l walltime=04:00:00\n\ncd $PBS_O_WORKDIR\n\nmodule purge\nmodule load craype-mic-knl intel/18.0.3 impi/18.0.3\nmpirun ./test_mpi.exe\n</code></pre>     - 4 nodes, 64 process per a node (MPI process of total: 256)</p>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#2-submit-interactive-job","title":"2. Submit Interactive Job","text":"<ul> <li>Not use #PBS but use <code>-I -A</code> option.     <pre><code>qsub -I -X -l select=1:ncpus=68:ompthread=1 -l walltime=12:00:00 -q normal -A {PBS option name}\n</code></pre></li> </ul>"},{"location":"Nurion_KISTI/01_HPC_overall_guide/#3-job-mornitoring","title":"3. Job Mornitoring","text":"<pre><code>showq # view queue\n\npbs_queue_check # view available queue list \n</code></pre> <pre><code>cp ./EXAMPLE.sh scratch/$USER_ID\ncds # scratch/$USER_ID\n\nqsub EXAMPLE.sh\nqstat -u [USER_ID] # see my queue(only R state)\n</code></pre>"},{"location":"Nurion_KISTI/02_HPC_job_tutorial/","title":"HPC Job Tutorials (KOR.)","text":"<p>Date: 2024.05.30 (Thur)  Writer: Chanyoung Ahn (cold-young)</p> <ul> <li>Reference: Nurion-guide</li> </ul>"},{"location":"Nurion_KISTI/02_HPC_job_tutorial/#submit-job","title":"Submit Job","text":"<ul> <li>Reference: Link</li> </ul> <p><pre><code>qsub -I -X -l select=1:ncpus=64:ompthreads=1 -l walltime=02:00:00 -q {QUEUE_NAME} -A {PBS OPTIONS}\n</code></pre> - <code>select</code>: The number of using nodes - <code>ncpus</code>: The number of using cores - <code>ompthreads</code>: The number of using threads  - <code>walltime</code>: runtime</p>"},{"location":"Nurion_KISTI/02_HPC_job_tutorial/#common-shell-commands","title":"Common shell commands","text":"<p><pre><code>motd # See basic notice and help\npbs_status # watch all nodes status \n\nqstat -u #view my jobs\nqstat -i -w -T -u [USER_ID]\nqstat -xf {JOBNUMBER}.pbs # view log \n</code></pre> - <code>qstat -u support</code>error! why cannot see my jobs?</p>"},{"location":"Nurion_KISTI/02_HPC_job_tutorial/#submit-nurion-interactive-job","title":"Submit NURION Interactive job","text":""},{"location":"Nurion_KISTI/02_HPC_job_tutorial/#write-job-script","title":"Write job script","text":"<p><pre><code>cd \ncd job_examples\n\nmpiifort hello.f90 -o hello.x\nvim mpi.sh\n</code></pre> - Compiler Option <pre><code>-O3 -fPIC -xMIC-AVX512 #KNL Node Recommand\n</code></pre></p> <ul> <li>mpi.sh <pre><code>#!/bin/sh\n#PBS -V\n#PBS -N mpi_job\n#PBS -q debug\n#PBS -A etc\n#PBS -l select=2:ncpus=32:mpiprocs=64\n#PBS -l walltime=04:00:00\n\ncd $PBS_O_WORKDIR\n\nmodule purge\nmodule load craype-mic-knl intel/18.0.3 impi/18.0.3\n\nmpirun ./hello.x\n</code></pre></li> </ul>"},{"location":"Nurion_KISTI/02_HPC_job_tutorial/#senddownload-files","title":"Send/Download files","text":"<pre><code>sftp [USER_ID]@nurion-dm.ksc.re.kr # Nurion\nsftp [USER_ID]@neuron-dm.ksc.re.kr # neuron\n\n# OTP PW: application\n# PW: USER_ID PW\n</code></pre> <pre><code># In ftp directory\ncd; ls; mkdir \n\n# In local directory\nlcd; lls; lmkdir \n\n# Send local &gt; nurion\nput [LOCAL_FILE]\n# mput [] [] ..\n\n# Download from nurion\nget [NURION_FILE]\n#mget [] [] .. \n</code></pre>"},{"location":"Nurion_KISTI/03_PaScaL_TDMA_env/","title":"Environment Setting for PaScaL_TDMA(KOR.)","text":"<p>Date: 2024.05.21 (Tue)  Writer: Chanyoung Ahn (cold-young)</p> <ul> <li>KOR version \uc791\uc131 \ud6c4, ENG version \uc791\uc131 \uc608\uc815.</li> <li>Reference: Nurion-guide</li> </ul>"},{"location":"Nurion_KISTI/03_PaScaL_TDMA_env/#prerequisites","title":"Prerequisites","text":"<ol> <li>My KSC - <code>Nurion</code> or <code>Neuron</code> / terminal</li> <li>See <code>module av</code>, <code>load</code> compiler/libraries</li> </ol> <pre><code>craype-network-opa\ncraype-mic-knl\nintel/oneapi_21.2\nimpi/oneapi_21.2\nxccels_lib/MUMPS_5.6.2\nxccels_lib/STARUMPACK_7.1.4\nxccels_lib/superlu_dist_8.1.2\n</code></pre> <pre><code># When start\nmodule load craype-network-opa craype-mic-knl \\\nintel/oneapi_21.2 impi/oneapi_21.2 \\\nxccels_lib/MUMPS_5.6.2 xccels_lib/STRUMPACK_7.1.4 \\\nxccels_lib/superlu_dist_8.1.2 \n\n# .. Or Add these commands in .bashrc\n# vim .bashrc\nmodule add craype-mic-knl\nmodule add intel/oneapi_21.2\nmodule add impi/oneapi_21.2\n\nmodule add xccels_lib/MUMPS_5.6.2\nmodule add xccels_lib/STRUMPACK_7.1.4\nmodule add xccels_lib/superlu_dist_8.1.2\n\nalias interactive='qsub -I -l select=1:ncpus=68:mpiprocs=68:ompthreads=1 -l walltime=4:00:00 -q debug -A etc'\n###\nsource .bash_rc\n\n\n# Check \nmodule list\n</code></pre> <ul> <li>We should submit any job and <code>interactive</code> alias in <code>scratch/USERID</code>.</li> </ul>"},{"location":"Nurion_KISTI/03_PaScaL_TDMA_env/#superlu-example","title":"SuperLU Example","text":"<pre><code>cds # cd scratch/$USERID\ncd ~/examples\n\n# Example: pddrive2.out\nmpiicc -I$INC_SUPERLUD -L$LIB_SUPERLUD -lsuperlu_dist dreadhb.c dcreate_matrix.c dcreate_matrix_perturbed.c pddrive2.c -o pddrive2.out\n\n# Example: test.out\nmpiicc -I$INC_SUPERLUD -L$LIB_SUPERLUD -lsuperlu_dist dreadhb.c dcreate_matrix.c dcreate_matrix_perturbed.c test.c -o test.out\n\n# Example: pddrive2.out with MKL_parallel link \nmpiicc -I$INC_SUPERLUD -L$LIB_SUPERLUD -lsuperlu_dist -L/apps/compiler/intel/oneapi_21.2/mkl/2021.2.0/lib/intel64/ -mkl=parallel dreadhb.c dcreate_matrix.c dcreate_matrix_perturbed.c pddrive2.c -o pddrive2.out\n</code></pre> <p><pre><code># Test exampels\nmpirun -np 4 ./OUTFILENAME.out -r 2 -c 2 tdm 16_new.rua \n</code></pre> - <code>-np</code>: the number of processors - <code>-r</code>: the number of processor grid row - <code>-c</code>: the number of processor grid column </p>"},{"location":"Nurion_KISTI/03_PaScaL_TDMA_env/#pascal_tdma-example","title":"PaScaL_TDMA Example","text":"<ul> <li>Reference: Docs <pre><code>cd examples\nmpirun -np 2 ./ex1_single.out\n\ncd ~/run\nmpirun -np 8 ./convection_3D.out ./PARA_INPUT.inp\n</code></pre></li> </ul>"},{"location":"Nurion_KISTI/03_PaScaL_TDMA_env/#makefile-tutorial","title":"Makefile Tutorial","text":""},{"location":"Nurion_KISTI/04_SuperLU_dist/","title":"Sparse Matrix Format (HB, CSR)","text":"<p>Date: 2024.05.29 (Wed)  Writer: Chanyoung Ahn (cold-young)</p>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#what-is-harwell-boeing-hb-format-rua-cua","title":"What is <code>Harwell-Boeing (HB)</code> format (<code>.rua / cua</code>)?","text":"<ul> <li>Reference: Link</li> <li> <p>It is common to use the same compressed column storage to represent the matrix in memory.</p> </li> </ul>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#16tdm_matrixrua","title":"<code>16tdm_matrix.rua</code>","text":"<pre><code>1:Tridiagonal matirx                                                      tdm     \n2:            17             1             2            10             4\n#    # of data lines  # of \ud589 \ub204\uc801 nnz #nnz \uc5f4 idx  # of A marix lines # of b matrix lines\n3:RUA                       16            16            46             0\n#                     # row           # column    # nnz \uc218\n4:(26I3)          (26I3)          (5E15.8)            (5E15.8)            \n5:F                          1             0\n6:  1  3  6  9 12 15 18 21 24 27 30 33 36 39 42 45 47\n7:  1  2  1  2  3  2  3  4  3  4  5  4  5  6  5  6  7  6  7  8  7  8  9  8  9 10\n8:  9 10 11 10 11 12 11 12 13 12 13 14 13 14 15 14 15 16 15 16\n9: 2.00000000e+00-1.00000000e+00-1.00000000e+00 2.00000000e+00-1.00000000e+00\n10:-1.00000000e+00 2.00000000e+00-1.00000000e+00-1.00000000e+00 2.00000000e+00\n...\n17:-1.00000000e+00 2.00000000e+00-1.00000000e+00-1.00000000e+00 2.00000000e+00\n18: 2.00000000e+00\n19: 2.00000000e+00 6.00000000e+00-9.00000000e+00 4.00000000e+00-9.00000000e+00\n...\n22: 9.00000000e+00\n</code></pre> <ul> <li>Line 1: Title / KEY</li> <li>Line 2: </li> <li>Total number of data lines: <code>17</code> (5 - 22 rows: 17 lines)</li> <li>Number of data lines for pointers: <code>1</code> (6 row: 1 line) </li> <li>Number of data lines for row or variable indices: <code>2</code> (7-8 row: 2 lines)</li> <li>Number of data lines for numerical values of matrix entries: <code>10</code> (9-18 row: 10 lines)</li> <li>PHSCRD Number of data lines for right hand side vectors, starting guesses, and solutions: <code>4</code> (19-22 row: 4 lines)</li> <li>Line 3: </li> <li>MATRIX TYPE: <code>RUA</code>(real, unsymmetric + square, assembled)</li> <li>Number of rows or variable: <code>16</code> </li> <li>Number of columns or variable: <code>16</code></li> <li>Number of nonzero entires: <code>46</code> (the number of elements in 7-8 row)</li> <li>Number of elemental matrix entries, \"assembled\":0 <code>0</code></li> <li>Line 4:</li> <li>(26I3): line break when write 26 element. Each element has 3 space. (6 row) / the accumulate number of row nnz</li> <li>(26I3): line break when write 26 element. Each element has 3 space. (7-8 row) / nnz colmn idx</li> <li>(5E15.8): line break when write 5 element. Each element has 15 space. (9-18 row) / A matrix</li> <li> <p>(5E15.8): line break when write 5 element. Each element has 15 space. (19-22 row) / b matrix </p> </li> <li> <p>Line 5 (PHSCRD&gt;0):</p> </li> <li>describes the right hand side information: <code>F</code>  right side information</li> <li>the number of right hand side: <code>1</code></li> <li> <p>number of row indices: <code>0</code></p> </li> <li> <p>6 row: point data / \ud589\ub82c \ud56d\ubaa9\uc758 \uc2dc\uc791 \uc704\uce58</p> </li> <li>7-8 row: index data / \uac01 \ud56d\ubaa9\uc5d0 \ud574\ub2f9\ub418\ub294 \ud589\uc758 \uc778\ub371\uc2a4</li> <li>9-18 row: A matrix</li> <li>19-22 row: b matrix</li> </ul> <p>All line length should be limited to 80 characters</p>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#examples-in-superlu-library","title":"Examples in SuperLU library","text":"<pre><code>cds # cd scratch/$USERID\ncd ~/examples\n\n# Example: pddrive2.out &amp; test.out\nmpiicc -I$INC_SUPERLUD -L$LIB_SUPERLUD -lsuperlu_dist dreadhb.c dcreate_matrix.c dcreate_matrix_perturbed.c \npddrive2.c -o pddrive2.out\n\n# Example: test.out (monitor b matrix)\nmpiicc -I$INC_SUPERLUD -L$LIB_SUPERLUD -lsuperlu_dist dreadhb.c dcreate_matrix.c dcreate_matrix_perturbed.c test.c -o test.out\n\n# Test exampels\nmpirun -np 4 ./OUTFILENAME.out -r 2 -c 2 tdm 16_new.rua \n</code></pre>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#dreadhbc","title":"<code>dreadhb.c</code> #","text":"<ul> <li>Read double precision matrix stored in Harwell-Boeing format. </li> </ul>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#dcreate_matrixc","title":"<code>dcreate_matrix.c</code>","text":"<ul> <li>Functions for reading the matrix with various formats</li> <li><code>DCREATE_MATRIX_POSTFIX</code> read the matrix from data file in different formats (.rua, .rb, .mtx, .dat, .ddatnh, and .bin)</li> </ul>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#decreate_matrix_perturbedc","title":"<code>decreate_matrix_perturbed.c</code>","text":"<ul> <li><code>DCREATE_MATRIX_PERTURBED</code> </li> <li>read the matrix from data file in Harwell-Boeing format, and distribute it to processors in a distributed compressed row format. It also generate the distributed true solution X and the right-hand side RHS.</li> <li> <p><code>iam</code>: <code>iam</code> means current processor's rank (ID)</p> </li> <li> <p><code>DCREATE_MATRIX</code> read the matrix from data file in Harwell-Boeing format, and distribute it to processors in a distributed compressed row format. It also generate the distributed true solution X and the right-hand.</p> </li> </ul>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#pddrive2c-and-testc","title":"<code>pddrive2.c</code> and <code>test.c</code>","text":"<ul> <li>These examples illustrate how to use PDGSSVX to solve systems repeatedly w/ the same sparsity pattern of matrix A.</li> </ul>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#testc-superlu-dist-benchmark","title":"<code>test.c</code>: SuperLU-dist Benchmark","text":"<ul> <li>Code Documentation</li> <li>Add Debug part in <code>pddrive2.c</code> <pre><code>    for(ii=0;ii&lt;4;ii++) {\n        printf(\"iam = %3d, x[%2d] = %15.10f\\n\", iam, ii, b[ii]);\n        printf(\"iam = %d, A.nnz_loc = %d\\n\", iam, Astore_temp-&gt;nnz_loc);\n        printf(\"iam = %d, A.fst_row = %d\\n\", iam, Astore_temp-&gt;fst_row);\n        printf(\"iam = %d, A.m_loc = %d\\n\", iam, Astore_temp-&gt;m_loc);\n    }\n</code></pre></li> </ul> <p><pre><code>// pdgssvx Function\npdgssvx(&amp;options, &amp;A, &amp;ScalePermstruct, b, ldb, nrhs, &amp;grid,\n            &amp;LUstruct, &amp;SOLVEstruct, berr, &amp;stat, &amp;info);\n</code></pre> - Reference - <code>pdgssvx</code> solves a system of linear equations \\(A \\times X = B\\), by using Gaussian elimination with \"static pivoting\" to compute the LU factorization of A.</p> <p></p>"},{"location":"Nurion_KISTI/04_SuperLU_dist/#csrcompressed-sparse-row-format","title":"CSR(Compressed Sparse Row) Format","text":"<ul> <li> <p>Reference: Link </p> </li> <li> <p><code>Sparse Matrix</code>: A matrix has less non-zero element. (&lt;&gt;<code>dense matrix</code>) </p> </li> <li>CSR Format:</li> <li><code>Data</code>: an array containing non-zero elements</li> <li><code>Rol</code>: an array containing column index of non-zero elements</li> <li><code>Row</code>: an array containing the number of non-zero elements before the nth row (accumulated value)</li> </ul>"}]}